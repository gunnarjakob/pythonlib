<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>gvpy.io API documentation</title>
<meta name="description" content="Module gvpy.io with in/out functions" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>gvpy.io</code></h1>
</header>
<section id="section-intro">
<p>Module gvpy.io with in/out functions</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-
&#34;&#34;&#34;Module gvpy.io with in/out functions
&#34;&#34;&#34;

import datetime as dt
import re
from pathlib import Path

import gsw
import numpy as np
import pandas as pd
import scipy.io as spio
import xarray as xr
from munch import munchify
from seabird.cnv import fCNV


def loadmat(filename, onevar=False, verbose=False):
    &#34;&#34;&#34;
    Load Matlab .mat files and return as dictionary with .dot-access.

    Parameters
    ----------
    filename : str
        Path to .mat file
    onevar : bool
        Set to true if there is only one variable in the mat file.

    Returns
    -------
    out : dict (Munch)
        Data in a munchified dictionary.
    &#34;&#34;&#34;

    def _check_keys(dict):
        &#34;&#34;&#34;
        checks if entries in dictionary are mat-objects. If yes
        todict is called to change them to nested dictionaries
        &#34;&#34;&#34;
        for key in dict:
            ni = np.size(dict[key])
            if ni &lt;= 1:
                if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):
                    dict[key] = _todict(dict[key])
            else:
                for i in range(0, ni):
                    if isinstance(dict[key][i], spio.matlab.mio5_params.mat_struct):
                        dict[key][i] = _todict(dict[key][i])
        return dict

    def _todict(matobj):
        &#34;&#34;&#34;
        A recursive function which constructs from matobjects nested dictionaries
        &#34;&#34;&#34;
        dict = {}
        for strg in matobj._fieldnames:
            elem = matobj.__dict__[strg]
            if isinstance(elem, spio.matlab.mio5_params.mat_struct):
                dict[strg] = _todict(elem)
            else:
                dict[strg] = elem
        return dict

    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)
    out = _check_keys(data)

    # Check if there is only one variable in the dataset. If so, directly
    # return only this variable as munchified dataset.
    if not onevar:
        dk = list(out.keys())
        actual_keys = [k for k in dk if k[:2] != &#34;__&#34;]
        if len(actual_keys) == 1:
            if verbose:
                print(&#34;found only one variable, returning munchified data structure&#34;)
            return munchify(out[actual_keys[0]])
        else:
            out2 = {}
            for k in actual_keys:
                out2[k] = out[k]
            return munchify(out2)

    # for legacy, keep the option in here as well.
    if onevar:
        # let&#39;s check if there is only one variable in there and return it
        kk = list(out.keys())
        outvars = []
        for k in kk:
            if k[:2] != &#34;__&#34;:
                outvars.append(k)
        if len(outvars) == 1:
            if verbose:
                print(&#34;returning munchified data structure&#34;)
            return munchify(out[outvars[0]])
        else:
            if verbose:
                print(&#34;found more than one var...&#34;)
            return out
    else:
        return out


def mtlb2datetime(matlab_datenum, strip_microseconds=False, strip_seconds=False):
    &#34;&#34;&#34;
    Convert Matlab datenum format to python datetime.
    
    This version also works for vector input and strips
    milliseconds if desired.

    Parameters
    ----------
    matlab_datenum : float or np.array
        Matlab time vector.
    strip_microseconds : bool
        Get rid of microseconds (optional)
    strip_seconds : bool
        Get rid of seconds (optional)

    Returns
    -------
    t : np.datetime64
        Time in numpy&#39;s datetime64 format.
    &#34;&#34;&#34;

    if np.size(matlab_datenum) == 1:
        day = dt.datetime.fromordinal(int(matlab_datenum))
        dayfrac = dt.timedelta(days=matlab_datenum % 1) - dt.timedelta(days=366)
        t1 = day + dayfrac
        if strip_microseconds and strip_seconds:
            t1 = dt.datetime.replace(t1, microsecond=0, second=0)
        elif strip_microseconds:
            t1 = dt.datetime.replace(t1, microsecond=0)

    else:
        t1 = np.ones_like(matlab_datenum) * np.nan
        t1 = t1.tolist()
        nonan = np.isfinite(matlab_datenum)
        md = matlab_datenum[nonan]
        day = [dt.datetime.fromordinal(int(tval)) for tval in md]
        dayfrac = [dt.timedelta(days=tval % 1) - dt.timedelta(days=366) for tval in md]
        tt = [day1 + dayfrac1 for day1, dayfrac1 in zip(day, dayfrac)]
        if strip_microseconds and strip_seconds:
            tt = [dt.datetime.replace(tval, microsecond=0, second=0) for tval in tt]
        elif strip_microseconds:
            tt = [dt.datetime.replace(tval, microsecond=0) for tval in tt]
        tt = [np.datetime64(ti) for ti in tt]
        xi = np.where(nonan)[0]
        for i, ii in enumerate(xi):
            t1[ii] = tt[i]
        xi = np.where(~nonan)[0]
        for i in xi:
            t1[i] = np.datetime64(&#34;nat&#34;)
        t1 = np.array(t1)

    return t1


def read_sbe_cnv(file, lat=0, lon=0):
    &#34;&#34;&#34;
    Read Seabird SBE37 .cnv file and return as xarray.Dataset.

    Parameters
    ----------
    file : str
        Complete path to .cnv file
    lat : float
        Latitude (used for gsw calculations). Defaults to zero.
    lon : float
        Longitude (used for gsw calculations). Defaults to zero.

    Returns
    -------
    mc : xarray.Dataset
        Microcat data as Dataset with some metadata in the attributes.
    &#34;&#34;&#34;
    # Read cnv file using Seabird package
    cnv = fCNV(file)

    # parse time
    mcyday = cnv[&#34;timeJV2&#34;]
    start_time_str_all = cnv.attributes[&#34;start_time&#34;]
    start_time_str = start_time_str_all.split(&#34;[&#34;)[0]
    base_year = pd.to_datetime(start_time_str).year
    mctime = yday1_to_datetime64(base_year, mcyday)
    # let&#39;s make sure the first time stamp we generated matches the string in the cnv file
    assert pd.to_datetime(np.datetime64(mctime[0], &#34;s&#34;)) == pd.to_datetime(
        start_time_str
    )

    # data vars
    dvars = {&#34;prdM&#34;: &#34;p&#34;, &#34;tv290C&#34;: &#34;t&#34;}
    mcdata = {}
    for k, di in dvars.items():
        if k in cnv.keys():
            # print(di, &#39;:&#39;, k)
            mcdata[di] = ([&#34;time&#34;], cnv[k])
    mc = xr.Dataset(data_vars=mcdata, coords={&#34;time&#34;: mctime})
    mc.attrs[&#34;file&#34;] = cnv.attributes[&#34;filename&#34;]
    mc.attrs[&#34;sbe_model&#34;] = cnv.attributes[&#34;sbe_model&#34;]
    # conductivity
    cvars = {&#34;cond0mS/cm&#34;: &#34;c&#34;, &#34;cond0S/m&#34;: &#34;c&#34;}
    for k, di in cvars.items():
        if k in cnv.keys():
            # convert from S/m to mS/cm as this is needed for gsw.SP_from_C
            if k == &#34;cond0S/m&#34;:
                conductivity = cnv[k] * 10
            else:
                conductivity = cnv[k]
            mc[di] = ([&#34;time&#34;], conductivity)

    # calculate oceanographic variables
    mc[&#34;SP&#34;] = ([&#34;time&#34;], gsw.SP_from_C(mc.c, mc.t, mc.p))
    if lat == 0 and lon == 0:
        print(
            &#34;warning: absolute salinity, conservative temperature\n&#34;,
            &#34;and density calculation may be inaccurate\n&#34;,
            &#34;due to missing latitude/longitude&#34;,
        )
    mc[&#34;SA&#34;] = ([&#34;time&#34;], gsw.SA_from_SP(mc.SP, mc.p, lat=lat, lon=lon))
    mc[&#34;CT&#34;] = ([&#34;time&#34;], gsw.CT_from_t(mc.SA, mc.t, mc.p))
    mc[&#34;sg0&#34;] = ([&#34;time&#34;], gsw.sigma0(mc.SA, mc.CT))

    # add attributes
    attributes = {
        &#34;p&#34;: dict(long_name=&#34;pressure&#34;, units=&#34;dbar&#34;),
        &#34;t&#34;: dict(long_name=&#34;in-situ temperature&#34;, units=&#34;°C&#34;),
        &#34;CT&#34;: dict(long_name=&#34;conservative temperature&#34;, units=&#34;°C&#34;),
        &#34;SA&#34;: dict(long_name=&#34;absolute salinity&#34;, units=r&#34;kg/m$^3$&#34;),
        &#34;c&#34;: dict(long_name=&#34;conductivity&#34;, units=&#34;mS/cm&#34;),
        &#34;SP&#34;: dict(long_name=&#34;practical salinity&#34;, units=&#34;&#34;),
        &#34;sg0&#34;: dict(long_name=r&#34;potential density $\sigma_0$&#34;, units=r&#34;kg/m$^3$&#34;),
    }
    for k, att in attributes.items():
        if k in list(mc.variables.keys()):
            mc[k].attrs = att

    return mc


def read_sadcp(ncfile):
    &#34;&#34;&#34;
    Read shipboard ADCP data file as produced by UHDAS.

    Parameters
    ----------
    ncfile : netcdf
        sADCP data from ship server.

    Returns
    -------
    xr.Dataset
        sADCP data in Dataset format.
    &#34;&#34;&#34;
    sadcp = xr.open_dataset(ncfile)

    mdepth = sadcp.depth.median(dim=&#34;time&#34;)
    sadcp = sadcp.drop(&#34;depth&#34;)
    # sadcp[&#39;depth&#39;] = ([&#39;depth_cell&#39;], mdepth)
    sadcp = sadcp.rename_dims({&#34;depth_cell&#34;: &#34;z&#34;})
    sadcp.coords[&#34;z&#34;] = ([&#34;z&#34;], mdepth)
    # Fix some attributes
    sadcp.z.attrs = dict(long_name=&#34;depth&#34;, units=&#34;m&#34;)
    sadcp.u.attrs = dict(long_name=&#34;u&#34;, units=&#34;m/s&#34;)
    sadcp.v.attrs = dict(long_name=&#34;v&#34;, units=&#34;m&#34;)
    sadcp.time.attrs = dict(long_name=&#34;time&#34;, units=&#34;&#34;)
    # Transpose (re-order dimensions)
    sadcp = sadcp.transpose(&#34;z&#34;, &#34;time&#34;)

    return sadcp


def mat2dataset(m1):
    &#34;&#34;&#34;
    Convert dictionary with data into xarray.Dataset

    Parameters
    ----------
    m1 : dict
        Dictionary with data, as ouput by gvpy.io.loadmat()

    Returns
    -------
    xr.Dataset
        Dataset with named variables
    &#34;&#34;&#34;
    k = m1.keys()

    varsint = []
    vars1d = []
    vars2d = []

    for ki in k:
        try:
            tmp = m1[ki].shape
            if len(tmp) == 1:
                vars1d.append(ki)
            elif len(tmp) == 2:
                vars2d.append(ki)
        except:
            tmp = None
            varsint.append(ki)
    # let&#39;s find probable dimensions. usually we have p or z for depth
    if &#34;z&#34; in k:
        jj = m1[&#34;z&#34;].shape[0]
    elif &#34;p&#34; in k:
        jj = m1[&#34;p&#34;].shape[0]
    elif &#34;P&#34; in k:
        jj = m1[&#34;P&#34;].shape[0]

    if &#34;lon&#34; in k:
        if len(m1[&#34;lon&#34;].shape) == 1:
            ii = m1[&#34;lon&#34;].shape[0]
    elif &#34;dnum&#34; in k:
        if len(m1[&#34;dnum&#34;].shape) == 1:
            ii = m1[&#34;dnum&#34;].shape[0]
    elif &#34;datenum&#34; in k:
        if len(m1[&#34;datenum&#34;].shape) == 1:
            ii = m1[&#34;datenum&#34;].shape[0]

    out = xr.Dataset(data_vars={&#34;dummy&#34;: ([&#34;z&#34;, &#34;x&#34;], np.ones((jj, ii)) * np.nan)})
    # get 1d variables
    for v in vars1d:
        if m1[v].shape[0] == ii:
            out[v] = ([&#34;x&#34;], m1[v])
        elif m1[v].shape[0] == jj:
            out[v] = ([&#34;z&#34;], m1[v])

    # convert the usual suspects into coordinates
    suspects = [&#34;lon&#34;, &#34;lat&#34;, &#34;p&#34;, &#34;z&#34;, &#34;depth&#34;, &#34;dep&#34;, &#34;P&#34;]
    for si in suspects:
        if si in vars1d:
            out.coords[si] = out[si]

    # convert time if possible
    for si in [&#34;datenum&#34;, &#34;dtnum&#34;, &#34;dnum&#34;, &#34;time&#34;]:
        if si in vars1d and np.nanmedian(m1[si]) &gt; 1e5:
            out.coords[&#34;time&#34;] = ([&#34;x&#34;], mtlb2datetime(m1[si]))
    # we have a problem if there is a variable called &#39;time&#39; in 2D.
    if &#34;time&#34; in vars2d:
        m1[&#39;time2d&#39;] = m1.pop(&#39;time&#39;)
        vars2d = [&#39;time2d&#39; if v==&#39;time&#39; else v for v in vars2d]

    # get 2d variables
    for v in vars2d:
        if m1[v].shape[0] == ii and m1[v].shape[1] == jj:
            out[v] = ([&#34;z&#34;, &#34;x&#34;], m1[v])
        elif m1[v].shape[0] == jj and m1[v].shape[1] == ii:
            out[v] = ([&#34;z&#34;, &#34;x&#34;], m1[v])

    # swap dim x for time if we have a time vector
    if &#34;time&#34; in out.coords:
        out = out.swap_dims({&#34;x&#34;: &#34;time&#34;})

    # drop dummy
    out = out.drop([&#34;dummy&#34;])

    # remove entries without time stamp
    if &#39;time&#39; in out.coords:
        out = out.where(~np.isnat(out.time), drop=True)
    return out


def str_to_datetime64(timestr):
    &#34;&#34;&#34;
    Convert date/time in str format to numpy&#39;s datetime64 format.
    
    Makes intermediate use of pandas datetime format, their string 
    conversion seems to be much more capable than numpy&#39;s.

    Parameters
    ----------
    timestr : str
        Date/time

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    return pd.to_datetime(timestr).to_datetime64()


def yday1_to_datetime64(baseyear, yday):
    &#34;&#34;&#34;
    Convert year day (starting at yday 1) to numpy&#39;s datetime64 format.

    Parameters
    ----------
    baseyear : int
        Base year
    yday : float
        Year day

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    base = dt.datetime(baseyear, 1, 1, 0, 0, 0)
    time = [base + dt.timedelta(days=ti) for ti in yday - 1]
    # convert to numpy datetime64
    time64 = np.array([np.datetime64(ti, &#34;ms&#34;) for ti in time])
    return time64


def yday0_to_datetime64(baseyear, yday):
    &#34;&#34;&#34;
    Convert year day (starting at yday 0) to numpy&#39;s datetime64 format.

    Parameters
    ----------
    baseyear : int
        Base year
    yday : float
        Year day

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    base = dt.datetime(baseyear, 1, 1, 0, 0, 0)
    time = [base + dt.timedelta(days=ti) for ti in yday]
    # convert to numpy datetime64
    time64 = np.array([np.datetime64(ti, &#34;ms&#34;) for ti in time])
    return time64


class ANTS(object):
    &#34;&#34;&#34;
    Reader for ANTS data files.
    
    These may be .vel, .bt or .sh files as
    generated by Andreas Thurnherr&#39;s various toolboxes.
    Borrows from Andreas&#39; matlab utilities.
    
    Parameters
    ----------
    filename : str or Path
        File to be read
    &#34;&#34;&#34;

    def __init__(self, filename):
        self.filename = filename
        if not isinstance(self.filename, Path):
            self.filename = Path(self.filename)

        with open(self.filename) as f:
            content = f.readlines()
        content = [x.strip() for x in content]

        p_error = re.compile(&#34;^#ANTS#ERROR#&#34;)
        p_param = re.compile(&#34;^#ANTS#PARAMS#&#34;)
        p_values = re.compile(r&#34;([\w\.]+){([^}]*)}&#34;)
        p_check_fields = re.compile(&#34;^#ANTS#FIELDS#&#34;)
        p_fields = re.compile(&#34;{([^}]*)}&#34;)
        p_data = re.compile(&#34;([^ \t]+)&#34;)

        test = []
        for c in content:
            if c[0] == &#34;#&#34;:
                if p_error.match(c):
                    print(&#34;error&#34;)
                elif p_param.match(c):
                    tmp = p_values.findall(c)
                    for tmpi in tmp:
                        parameter, value = tmpi
                        if _is_number(value):
                            value = float(value)
                        setattr(self, parameter.replace(&#34;.&#34;, &#34;_&#34;), value)
                elif p_check_fields.match(c):
                    # we&#39;ll overwrite any previous results from lines like this
                    fieldnames = []
                    tmp = p_fields.findall(c)
                    for tmpi in tmp:
                        fieldnames.append(tmpi.replace(&#34;.&#34;, &#34;_&#34;))
            elif _is_number(c[:2]):
                tmp = p_data.findall(c)  # list of numbers as strings
                tmp = np.array([float(i) for i in tmp])  # convert to floats
                test.append(tmp)  # add to temporary list
        all_data = np.vstack(test)
        for i, f in enumerate(fieldnames):
            setattr(self, f, all_data[:, i])

    def to_xarray(self):
        &#34;&#34;&#34;
        Convert ANTS object to xarray.Dataset.

        Returns
        -------
        ds : xarray.Dataset
            xarray data structure
        &#34;&#34;&#34;
        return self._to_xarray(self)

    def _to_xarray(self):
        &#34;&#34;&#34;
        Convert ANTS object to xarray.Dataset.

        Returns
        -------
        ds : xarray.Dataset
            xarray data structure
        &#34;&#34;&#34;
        all_attrs = dir(self)
        cleaned_attrs = [x for x in all_attrs if x[0] != &#34;_&#34;]
        cdic = {ci: [] for ci in cleaned_attrs}
        ds = xr.Dataset()

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, np.ndarray):
                cdic.pop(ci)
                ds[ci] = ([&#34;n&#34;], a)

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, float):
                cdic.pop(ci)
                ds[ci] = ([&#34;cast&#34;], [a])

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, str):
                cdic.pop(ci)
                ds.attrs[ci] = a

        # add source file name as attribute to dataset
        ds.attrs[&#34;filename&#34;] = self.filename

        # change dim, coord based on file suffix
        if self.filename.suffix == &#34;.VKE&#34;:
            ds = ds.swap_dims({&#34;n&#34;: &#34;depth&#34;})
            ds.coords[&#34;depth&#34;] = ds.depth
        if self.filename.suffix == &#34;.wprof&#34;:
            ds = ds.swap_dims({&#34;n&#34;: &#34;depth&#34;})
            for c in [&#34;depth&#34;, &#34;hab&#34;, &#34;dc_depth&#34;, &#34;uc_depth&#34;]:
                if c in ds:
                    ds.coords[c] = ds[c]

        return ds


def results_to_latex(res, file):
    &#34;&#34;&#34;Write dictionary with results to a latex file.

    In your latex document, use \\include{FileName} to read the document
    and then call variables as for e.g. \\OverallResult. Note that the file needs
    to be in the same directory as the main tex document.

    Parameters
    ----------
    res : dict
        Dictionary with results as values and new latex variables as keys.
    file : Path object
        Path to latex file to be generated.
    &#34;&#34;&#34;

    def newcommand(name, val):
        &#34;&#34;&#34;Format name and val into latex command&#34;&#34;&#34;
        fmt = &#39;\\newcommand{{\\{name}}}[0]{{{action}}}&#39;
        cmd = fmt.format(name=name, action=val)
        print(cmd)
        return cmd + &#39;\n&#39;

    cmds = []
    for key, values in res.items():
        cmds+=newcommand(key, values)

    with open(file, &#39;a&#39;) as fh:
        for cmd in cmds:
            fh.write(cmd)


def _is_number(s):
    &#34;&#34;&#34;
    Check if string can be converted to a float.

    Parameters
    ----------
    s : str
        string

    Returns
    -------
    out : bool
        True if string can be converted to float, else False.
    &#34;&#34;&#34;
    try:
        float(s)
        return True
    except ValueError:
        return False</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="gvpy.io.loadmat"><code class="name flex">
<span>def <span class="ident">loadmat</span></span>(<span>filename, onevar=False, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Load Matlab .mat files and return as dictionary with .dot-access.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Path to .mat file</dd>
<dt><strong><code>onevar</code></strong> :&ensp;<code>bool</code></dt>
<dd>Set to true if there is only one variable in the mat file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>dict (Munch)</code></dt>
<dd>Data in a munchified dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loadmat(filename, onevar=False, verbose=False):
    &#34;&#34;&#34;
    Load Matlab .mat files and return as dictionary with .dot-access.

    Parameters
    ----------
    filename : str
        Path to .mat file
    onevar : bool
        Set to true if there is only one variable in the mat file.

    Returns
    -------
    out : dict (Munch)
        Data in a munchified dictionary.
    &#34;&#34;&#34;

    def _check_keys(dict):
        &#34;&#34;&#34;
        checks if entries in dictionary are mat-objects. If yes
        todict is called to change them to nested dictionaries
        &#34;&#34;&#34;
        for key in dict:
            ni = np.size(dict[key])
            if ni &lt;= 1:
                if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):
                    dict[key] = _todict(dict[key])
            else:
                for i in range(0, ni):
                    if isinstance(dict[key][i], spio.matlab.mio5_params.mat_struct):
                        dict[key][i] = _todict(dict[key][i])
        return dict

    def _todict(matobj):
        &#34;&#34;&#34;
        A recursive function which constructs from matobjects nested dictionaries
        &#34;&#34;&#34;
        dict = {}
        for strg in matobj._fieldnames:
            elem = matobj.__dict__[strg]
            if isinstance(elem, spio.matlab.mio5_params.mat_struct):
                dict[strg] = _todict(elem)
            else:
                dict[strg] = elem
        return dict

    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)
    out = _check_keys(data)

    # Check if there is only one variable in the dataset. If so, directly
    # return only this variable as munchified dataset.
    if not onevar:
        dk = list(out.keys())
        actual_keys = [k for k in dk if k[:2] != &#34;__&#34;]
        if len(actual_keys) == 1:
            if verbose:
                print(&#34;found only one variable, returning munchified data structure&#34;)
            return munchify(out[actual_keys[0]])
        else:
            out2 = {}
            for k in actual_keys:
                out2[k] = out[k]
            return munchify(out2)

    # for legacy, keep the option in here as well.
    if onevar:
        # let&#39;s check if there is only one variable in there and return it
        kk = list(out.keys())
        outvars = []
        for k in kk:
            if k[:2] != &#34;__&#34;:
                outvars.append(k)
        if len(outvars) == 1:
            if verbose:
                print(&#34;returning munchified data structure&#34;)
            return munchify(out[outvars[0]])
        else:
            if verbose:
                print(&#34;found more than one var...&#34;)
            return out
    else:
        return out</code></pre>
</details>
</dd>
<dt id="gvpy.io.mat2dataset"><code class="name flex">
<span>def <span class="ident">mat2dataset</span></span>(<span>m1)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert dictionary with data into xarray.Dataset</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>m1</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary with data, as ouput by gvpy.io.loadmat()</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.Dataset</code></dt>
<dd>Dataset with named variables</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mat2dataset(m1):
    &#34;&#34;&#34;
    Convert dictionary with data into xarray.Dataset

    Parameters
    ----------
    m1 : dict
        Dictionary with data, as ouput by gvpy.io.loadmat()

    Returns
    -------
    xr.Dataset
        Dataset with named variables
    &#34;&#34;&#34;
    k = m1.keys()

    varsint = []
    vars1d = []
    vars2d = []

    for ki in k:
        try:
            tmp = m1[ki].shape
            if len(tmp) == 1:
                vars1d.append(ki)
            elif len(tmp) == 2:
                vars2d.append(ki)
        except:
            tmp = None
            varsint.append(ki)
    # let&#39;s find probable dimensions. usually we have p or z for depth
    if &#34;z&#34; in k:
        jj = m1[&#34;z&#34;].shape[0]
    elif &#34;p&#34; in k:
        jj = m1[&#34;p&#34;].shape[0]
    elif &#34;P&#34; in k:
        jj = m1[&#34;P&#34;].shape[0]

    if &#34;lon&#34; in k:
        if len(m1[&#34;lon&#34;].shape) == 1:
            ii = m1[&#34;lon&#34;].shape[0]
    elif &#34;dnum&#34; in k:
        if len(m1[&#34;dnum&#34;].shape) == 1:
            ii = m1[&#34;dnum&#34;].shape[0]
    elif &#34;datenum&#34; in k:
        if len(m1[&#34;datenum&#34;].shape) == 1:
            ii = m1[&#34;datenum&#34;].shape[0]

    out = xr.Dataset(data_vars={&#34;dummy&#34;: ([&#34;z&#34;, &#34;x&#34;], np.ones((jj, ii)) * np.nan)})
    # get 1d variables
    for v in vars1d:
        if m1[v].shape[0] == ii:
            out[v] = ([&#34;x&#34;], m1[v])
        elif m1[v].shape[0] == jj:
            out[v] = ([&#34;z&#34;], m1[v])

    # convert the usual suspects into coordinates
    suspects = [&#34;lon&#34;, &#34;lat&#34;, &#34;p&#34;, &#34;z&#34;, &#34;depth&#34;, &#34;dep&#34;, &#34;P&#34;]
    for si in suspects:
        if si in vars1d:
            out.coords[si] = out[si]

    # convert time if possible
    for si in [&#34;datenum&#34;, &#34;dtnum&#34;, &#34;dnum&#34;, &#34;time&#34;]:
        if si in vars1d and np.nanmedian(m1[si]) &gt; 1e5:
            out.coords[&#34;time&#34;] = ([&#34;x&#34;], mtlb2datetime(m1[si]))
    # we have a problem if there is a variable called &#39;time&#39; in 2D.
    if &#34;time&#34; in vars2d:
        m1[&#39;time2d&#39;] = m1.pop(&#39;time&#39;)
        vars2d = [&#39;time2d&#39; if v==&#39;time&#39; else v for v in vars2d]

    # get 2d variables
    for v in vars2d:
        if m1[v].shape[0] == ii and m1[v].shape[1] == jj:
            out[v] = ([&#34;z&#34;, &#34;x&#34;], m1[v])
        elif m1[v].shape[0] == jj and m1[v].shape[1] == ii:
            out[v] = ([&#34;z&#34;, &#34;x&#34;], m1[v])

    # swap dim x for time if we have a time vector
    if &#34;time&#34; in out.coords:
        out = out.swap_dims({&#34;x&#34;: &#34;time&#34;})

    # drop dummy
    out = out.drop([&#34;dummy&#34;])

    # remove entries without time stamp
    if &#39;time&#39; in out.coords:
        out = out.where(~np.isnat(out.time), drop=True)
    return out</code></pre>
</details>
</dd>
<dt id="gvpy.io.mtlb2datetime"><code class="name flex">
<span>def <span class="ident">mtlb2datetime</span></span>(<span>matlab_datenum, strip_microseconds=False, strip_seconds=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert Matlab datenum format to python datetime.</p>
<p>This version also works for vector input and strips
milliseconds if desired.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matlab_datenum</code></strong> :&ensp;<code>float</code> or <code>np.array</code></dt>
<dd>Matlab time vector.</dd>
<dt><strong><code>strip_microseconds</code></strong> :&ensp;<code>bool</code></dt>
<dd>Get rid of microseconds (optional)</dd>
<dt><strong><code>strip_seconds</code></strong> :&ensp;<code>bool</code></dt>
<dd>Get rid of seconds (optional)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>t</code></strong> :&ensp;<code>np.datetime64</code></dt>
<dd>Time in numpy's datetime64 format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mtlb2datetime(matlab_datenum, strip_microseconds=False, strip_seconds=False):
    &#34;&#34;&#34;
    Convert Matlab datenum format to python datetime.
    
    This version also works for vector input and strips
    milliseconds if desired.

    Parameters
    ----------
    matlab_datenum : float or np.array
        Matlab time vector.
    strip_microseconds : bool
        Get rid of microseconds (optional)
    strip_seconds : bool
        Get rid of seconds (optional)

    Returns
    -------
    t : np.datetime64
        Time in numpy&#39;s datetime64 format.
    &#34;&#34;&#34;

    if np.size(matlab_datenum) == 1:
        day = dt.datetime.fromordinal(int(matlab_datenum))
        dayfrac = dt.timedelta(days=matlab_datenum % 1) - dt.timedelta(days=366)
        t1 = day + dayfrac
        if strip_microseconds and strip_seconds:
            t1 = dt.datetime.replace(t1, microsecond=0, second=0)
        elif strip_microseconds:
            t1 = dt.datetime.replace(t1, microsecond=0)

    else:
        t1 = np.ones_like(matlab_datenum) * np.nan
        t1 = t1.tolist()
        nonan = np.isfinite(matlab_datenum)
        md = matlab_datenum[nonan]
        day = [dt.datetime.fromordinal(int(tval)) for tval in md]
        dayfrac = [dt.timedelta(days=tval % 1) - dt.timedelta(days=366) for tval in md]
        tt = [day1 + dayfrac1 for day1, dayfrac1 in zip(day, dayfrac)]
        if strip_microseconds and strip_seconds:
            tt = [dt.datetime.replace(tval, microsecond=0, second=0) for tval in tt]
        elif strip_microseconds:
            tt = [dt.datetime.replace(tval, microsecond=0) for tval in tt]
        tt = [np.datetime64(ti) for ti in tt]
        xi = np.where(nonan)[0]
        for i, ii in enumerate(xi):
            t1[ii] = tt[i]
        xi = np.where(~nonan)[0]
        for i in xi:
            t1[i] = np.datetime64(&#34;nat&#34;)
        t1 = np.array(t1)

    return t1</code></pre>
</details>
</dd>
<dt id="gvpy.io.read_sadcp"><code class="name flex">
<span>def <span class="ident">read_sadcp</span></span>(<span>ncfile)</span>
</code></dt>
<dd>
<div class="desc"><p>Read shipboard ADCP data file as produced by UHDAS.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ncfile</code></strong> :&ensp;<code>netcdf</code></dt>
<dd>sADCP data from ship server.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xr.Dataset</code></dt>
<dd>sADCP data in Dataset format.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_sadcp(ncfile):
    &#34;&#34;&#34;
    Read shipboard ADCP data file as produced by UHDAS.

    Parameters
    ----------
    ncfile : netcdf
        sADCP data from ship server.

    Returns
    -------
    xr.Dataset
        sADCP data in Dataset format.
    &#34;&#34;&#34;
    sadcp = xr.open_dataset(ncfile)

    mdepth = sadcp.depth.median(dim=&#34;time&#34;)
    sadcp = sadcp.drop(&#34;depth&#34;)
    # sadcp[&#39;depth&#39;] = ([&#39;depth_cell&#39;], mdepth)
    sadcp = sadcp.rename_dims({&#34;depth_cell&#34;: &#34;z&#34;})
    sadcp.coords[&#34;z&#34;] = ([&#34;z&#34;], mdepth)
    # Fix some attributes
    sadcp.z.attrs = dict(long_name=&#34;depth&#34;, units=&#34;m&#34;)
    sadcp.u.attrs = dict(long_name=&#34;u&#34;, units=&#34;m/s&#34;)
    sadcp.v.attrs = dict(long_name=&#34;v&#34;, units=&#34;m&#34;)
    sadcp.time.attrs = dict(long_name=&#34;time&#34;, units=&#34;&#34;)
    # Transpose (re-order dimensions)
    sadcp = sadcp.transpose(&#34;z&#34;, &#34;time&#34;)

    return sadcp</code></pre>
</details>
</dd>
<dt id="gvpy.io.read_sbe_cnv"><code class="name flex">
<span>def <span class="ident">read_sbe_cnv</span></span>(<span>file, lat=0, lon=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Read Seabird SBE37 .cnv file and return as xarray.Dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str</code></dt>
<dd>Complete path to .cnv file</dd>
<dt><strong><code>lat</code></strong> :&ensp;<code>float</code></dt>
<dd>Latitude (used for gsw calculations). Defaults to zero.</dd>
<dt><strong><code>lon</code></strong> :&ensp;<code>float</code></dt>
<dd>Longitude (used for gsw calculations). Defaults to zero.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mc</code></strong> :&ensp;<code>xarray.Dataset</code></dt>
<dd>Microcat data as Dataset with some metadata in the attributes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_sbe_cnv(file, lat=0, lon=0):
    &#34;&#34;&#34;
    Read Seabird SBE37 .cnv file and return as xarray.Dataset.

    Parameters
    ----------
    file : str
        Complete path to .cnv file
    lat : float
        Latitude (used for gsw calculations). Defaults to zero.
    lon : float
        Longitude (used for gsw calculations). Defaults to zero.

    Returns
    -------
    mc : xarray.Dataset
        Microcat data as Dataset with some metadata in the attributes.
    &#34;&#34;&#34;
    # Read cnv file using Seabird package
    cnv = fCNV(file)

    # parse time
    mcyday = cnv[&#34;timeJV2&#34;]
    start_time_str_all = cnv.attributes[&#34;start_time&#34;]
    start_time_str = start_time_str_all.split(&#34;[&#34;)[0]
    base_year = pd.to_datetime(start_time_str).year
    mctime = yday1_to_datetime64(base_year, mcyday)
    # let&#39;s make sure the first time stamp we generated matches the string in the cnv file
    assert pd.to_datetime(np.datetime64(mctime[0], &#34;s&#34;)) == pd.to_datetime(
        start_time_str
    )

    # data vars
    dvars = {&#34;prdM&#34;: &#34;p&#34;, &#34;tv290C&#34;: &#34;t&#34;}
    mcdata = {}
    for k, di in dvars.items():
        if k in cnv.keys():
            # print(di, &#39;:&#39;, k)
            mcdata[di] = ([&#34;time&#34;], cnv[k])
    mc = xr.Dataset(data_vars=mcdata, coords={&#34;time&#34;: mctime})
    mc.attrs[&#34;file&#34;] = cnv.attributes[&#34;filename&#34;]
    mc.attrs[&#34;sbe_model&#34;] = cnv.attributes[&#34;sbe_model&#34;]
    # conductivity
    cvars = {&#34;cond0mS/cm&#34;: &#34;c&#34;, &#34;cond0S/m&#34;: &#34;c&#34;}
    for k, di in cvars.items():
        if k in cnv.keys():
            # convert from S/m to mS/cm as this is needed for gsw.SP_from_C
            if k == &#34;cond0S/m&#34;:
                conductivity = cnv[k] * 10
            else:
                conductivity = cnv[k]
            mc[di] = ([&#34;time&#34;], conductivity)

    # calculate oceanographic variables
    mc[&#34;SP&#34;] = ([&#34;time&#34;], gsw.SP_from_C(mc.c, mc.t, mc.p))
    if lat == 0 and lon == 0:
        print(
            &#34;warning: absolute salinity, conservative temperature\n&#34;,
            &#34;and density calculation may be inaccurate\n&#34;,
            &#34;due to missing latitude/longitude&#34;,
        )
    mc[&#34;SA&#34;] = ([&#34;time&#34;], gsw.SA_from_SP(mc.SP, mc.p, lat=lat, lon=lon))
    mc[&#34;CT&#34;] = ([&#34;time&#34;], gsw.CT_from_t(mc.SA, mc.t, mc.p))
    mc[&#34;sg0&#34;] = ([&#34;time&#34;], gsw.sigma0(mc.SA, mc.CT))

    # add attributes
    attributes = {
        &#34;p&#34;: dict(long_name=&#34;pressure&#34;, units=&#34;dbar&#34;),
        &#34;t&#34;: dict(long_name=&#34;in-situ temperature&#34;, units=&#34;°C&#34;),
        &#34;CT&#34;: dict(long_name=&#34;conservative temperature&#34;, units=&#34;°C&#34;),
        &#34;SA&#34;: dict(long_name=&#34;absolute salinity&#34;, units=r&#34;kg/m$^3$&#34;),
        &#34;c&#34;: dict(long_name=&#34;conductivity&#34;, units=&#34;mS/cm&#34;),
        &#34;SP&#34;: dict(long_name=&#34;practical salinity&#34;, units=&#34;&#34;),
        &#34;sg0&#34;: dict(long_name=r&#34;potential density $\sigma_0$&#34;, units=r&#34;kg/m$^3$&#34;),
    }
    for k, att in attributes.items():
        if k in list(mc.variables.keys()):
            mc[k].attrs = att

    return mc</code></pre>
</details>
</dd>
<dt id="gvpy.io.results_to_latex"><code class="name flex">
<span>def <span class="ident">results_to_latex</span></span>(<span>res, file)</span>
</code></dt>
<dd>
<div class="desc"><p>Write dictionary with results to a latex file.</p>
<p>In your latex document, use \include{FileName} to read the document
and then call variables as for e.g. \OverallResult. Note that the file needs
to be in the same directory as the main tex document.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>res</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary with results as values and new latex variables as keys.</dd>
<dt><strong><code>file</code></strong> :&ensp;<code>Path object</code></dt>
<dd>Path to latex file to be generated.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def results_to_latex(res, file):
    &#34;&#34;&#34;Write dictionary with results to a latex file.

    In your latex document, use \\include{FileName} to read the document
    and then call variables as for e.g. \\OverallResult. Note that the file needs
    to be in the same directory as the main tex document.

    Parameters
    ----------
    res : dict
        Dictionary with results as values and new latex variables as keys.
    file : Path object
        Path to latex file to be generated.
    &#34;&#34;&#34;

    def newcommand(name, val):
        &#34;&#34;&#34;Format name and val into latex command&#34;&#34;&#34;
        fmt = &#39;\\newcommand{{\\{name}}}[0]{{{action}}}&#39;
        cmd = fmt.format(name=name, action=val)
        print(cmd)
        return cmd + &#39;\n&#39;

    cmds = []
    for key, values in res.items():
        cmds+=newcommand(key, values)

    with open(file, &#39;a&#39;) as fh:
        for cmd in cmds:
            fh.write(cmd)</code></pre>
</details>
</dd>
<dt id="gvpy.io.str_to_datetime64"><code class="name flex">
<span>def <span class="ident">str_to_datetime64</span></span>(<span>timestr)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert date/time in str format to numpy's datetime64 format.</p>
<p>Makes intermediate use of pandas datetime format, their string
conversion seems to be much more capable than numpy's.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>timestr</code></strong> :&ensp;<code>str</code></dt>
<dd>Date/time</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>time</code></strong> :&ensp;<code>np.datetime64</code></dt>
<dd>Time in numpy datetime64 format</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def str_to_datetime64(timestr):
    &#34;&#34;&#34;
    Convert date/time in str format to numpy&#39;s datetime64 format.
    
    Makes intermediate use of pandas datetime format, their string 
    conversion seems to be much more capable than numpy&#39;s.

    Parameters
    ----------
    timestr : str
        Date/time

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    return pd.to_datetime(timestr).to_datetime64()</code></pre>
</details>
</dd>
<dt id="gvpy.io.yday0_to_datetime64"><code class="name flex">
<span>def <span class="ident">yday0_to_datetime64</span></span>(<span>baseyear, yday)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert year day (starting at yday 0) to numpy's datetime64 format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>baseyear</code></strong> :&ensp;<code>int</code></dt>
<dd>Base year</dd>
<dt><strong><code>yday</code></strong> :&ensp;<code>float</code></dt>
<dd>Year day</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>time</code></strong> :&ensp;<code>np.datetime64</code></dt>
<dd>Time in numpy datetime64 format</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yday0_to_datetime64(baseyear, yday):
    &#34;&#34;&#34;
    Convert year day (starting at yday 0) to numpy&#39;s datetime64 format.

    Parameters
    ----------
    baseyear : int
        Base year
    yday : float
        Year day

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    base = dt.datetime(baseyear, 1, 1, 0, 0, 0)
    time = [base + dt.timedelta(days=ti) for ti in yday]
    # convert to numpy datetime64
    time64 = np.array([np.datetime64(ti, &#34;ms&#34;) for ti in time])
    return time64</code></pre>
</details>
</dd>
<dt id="gvpy.io.yday1_to_datetime64"><code class="name flex">
<span>def <span class="ident">yday1_to_datetime64</span></span>(<span>baseyear, yday)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert year day (starting at yday 1) to numpy's datetime64 format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>baseyear</code></strong> :&ensp;<code>int</code></dt>
<dd>Base year</dd>
<dt><strong><code>yday</code></strong> :&ensp;<code>float</code></dt>
<dd>Year day</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>time</code></strong> :&ensp;<code>np.datetime64</code></dt>
<dd>Time in numpy datetime64 format</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yday1_to_datetime64(baseyear, yday):
    &#34;&#34;&#34;
    Convert year day (starting at yday 1) to numpy&#39;s datetime64 format.

    Parameters
    ----------
    baseyear : int
        Base year
    yday : float
        Year day

    Returns
    -------
    time : np.datetime64
        Time in numpy datetime64 format
    &#34;&#34;&#34;
    base = dt.datetime(baseyear, 1, 1, 0, 0, 0)
    time = [base + dt.timedelta(days=ti) for ti in yday - 1]
    # convert to numpy datetime64
    time64 = np.array([np.datetime64(ti, &#34;ms&#34;) for ti in time])
    return time64</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="gvpy.io.ANTS"><code class="flex name class">
<span>class <span class="ident">ANTS</span></span>
<span>(</span><span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Reader for ANTS data files.</p>
<p>These may be .vel, .bt or .sh files as
generated by Andreas Thurnherr's various toolboxes.
Borrows from Andreas' matlab utilities.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code> or <code>Path</code></dt>
<dd>File to be read</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ANTS(object):
    &#34;&#34;&#34;
    Reader for ANTS data files.
    
    These may be .vel, .bt or .sh files as
    generated by Andreas Thurnherr&#39;s various toolboxes.
    Borrows from Andreas&#39; matlab utilities.
    
    Parameters
    ----------
    filename : str or Path
        File to be read
    &#34;&#34;&#34;

    def __init__(self, filename):
        self.filename = filename
        if not isinstance(self.filename, Path):
            self.filename = Path(self.filename)

        with open(self.filename) as f:
            content = f.readlines()
        content = [x.strip() for x in content]

        p_error = re.compile(&#34;^#ANTS#ERROR#&#34;)
        p_param = re.compile(&#34;^#ANTS#PARAMS#&#34;)
        p_values = re.compile(r&#34;([\w\.]+){([^}]*)}&#34;)
        p_check_fields = re.compile(&#34;^#ANTS#FIELDS#&#34;)
        p_fields = re.compile(&#34;{([^}]*)}&#34;)
        p_data = re.compile(&#34;([^ \t]+)&#34;)

        test = []
        for c in content:
            if c[0] == &#34;#&#34;:
                if p_error.match(c):
                    print(&#34;error&#34;)
                elif p_param.match(c):
                    tmp = p_values.findall(c)
                    for tmpi in tmp:
                        parameter, value = tmpi
                        if _is_number(value):
                            value = float(value)
                        setattr(self, parameter.replace(&#34;.&#34;, &#34;_&#34;), value)
                elif p_check_fields.match(c):
                    # we&#39;ll overwrite any previous results from lines like this
                    fieldnames = []
                    tmp = p_fields.findall(c)
                    for tmpi in tmp:
                        fieldnames.append(tmpi.replace(&#34;.&#34;, &#34;_&#34;))
            elif _is_number(c[:2]):
                tmp = p_data.findall(c)  # list of numbers as strings
                tmp = np.array([float(i) for i in tmp])  # convert to floats
                test.append(tmp)  # add to temporary list
        all_data = np.vstack(test)
        for i, f in enumerate(fieldnames):
            setattr(self, f, all_data[:, i])

    def to_xarray(self):
        &#34;&#34;&#34;
        Convert ANTS object to xarray.Dataset.

        Returns
        -------
        ds : xarray.Dataset
            xarray data structure
        &#34;&#34;&#34;
        return self._to_xarray(self)

    def _to_xarray(self):
        &#34;&#34;&#34;
        Convert ANTS object to xarray.Dataset.

        Returns
        -------
        ds : xarray.Dataset
            xarray data structure
        &#34;&#34;&#34;
        all_attrs = dir(self)
        cleaned_attrs = [x for x in all_attrs if x[0] != &#34;_&#34;]
        cdic = {ci: [] for ci in cleaned_attrs}
        ds = xr.Dataset()

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, np.ndarray):
                cdic.pop(ci)
                ds[ci] = ([&#34;n&#34;], a)

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, float):
                cdic.pop(ci)
                ds[ci] = ([&#34;cast&#34;], [a])

        ck = list(cdic.keys())
        for ci in ck:
            a = getattr(self, ci)
            if isinstance(a, str):
                cdic.pop(ci)
                ds.attrs[ci] = a

        # add source file name as attribute to dataset
        ds.attrs[&#34;filename&#34;] = self.filename

        # change dim, coord based on file suffix
        if self.filename.suffix == &#34;.VKE&#34;:
            ds = ds.swap_dims({&#34;n&#34;: &#34;depth&#34;})
            ds.coords[&#34;depth&#34;] = ds.depth
        if self.filename.suffix == &#34;.wprof&#34;:
            ds = ds.swap_dims({&#34;n&#34;: &#34;depth&#34;})
            for c in [&#34;depth&#34;, &#34;hab&#34;, &#34;dc_depth&#34;, &#34;uc_depth&#34;]:
                if c in ds:
                    ds.coords[c] = ds[c]

        return ds</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="gvpy.io.ANTS.to_xarray"><code class="name flex">
<span>def <span class="ident">to_xarray</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Convert ANTS object to xarray.Dataset.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>ds</code></strong> :&ensp;<code>xarray.Dataset</code></dt>
<dd>xarray data structure</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_xarray(self):
    &#34;&#34;&#34;
    Convert ANTS object to xarray.Dataset.

    Returns
    -------
    ds : xarray.Dataset
        xarray data structure
    &#34;&#34;&#34;
    return self._to_xarray(self)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="gvpy" href="index.html">gvpy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="gvpy.io.loadmat" href="#gvpy.io.loadmat">loadmat</a></code></li>
<li><code><a title="gvpy.io.mat2dataset" href="#gvpy.io.mat2dataset">mat2dataset</a></code></li>
<li><code><a title="gvpy.io.mtlb2datetime" href="#gvpy.io.mtlb2datetime">mtlb2datetime</a></code></li>
<li><code><a title="gvpy.io.read_sadcp" href="#gvpy.io.read_sadcp">read_sadcp</a></code></li>
<li><code><a title="gvpy.io.read_sbe_cnv" href="#gvpy.io.read_sbe_cnv">read_sbe_cnv</a></code></li>
<li><code><a title="gvpy.io.results_to_latex" href="#gvpy.io.results_to_latex">results_to_latex</a></code></li>
<li><code><a title="gvpy.io.str_to_datetime64" href="#gvpy.io.str_to_datetime64">str_to_datetime64</a></code></li>
<li><code><a title="gvpy.io.yday0_to_datetime64" href="#gvpy.io.yday0_to_datetime64">yday0_to_datetime64</a></code></li>
<li><code><a title="gvpy.io.yday1_to_datetime64" href="#gvpy.io.yday1_to_datetime64">yday1_to_datetime64</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="gvpy.io.ANTS" href="#gvpy.io.ANTS">ANTS</a></code></h4>
<ul class="">
<li><code><a title="gvpy.io.ANTS.to_xarray" href="#gvpy.io.ANTS.to_xarray">to_xarray</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>